---
title: "forecastML Overview"
author: "Nickalus Redell, nickalusredell@gmail.com"
date: "`r lubridate::today()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{forecastML Overview}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


![](forecastML_logo.png){width=150px, height=160px}

```{r, eval = FALSE, echo = FALSE, out.extra = 'style = "position:absolute; top:0; right:0; padding:5px; width: 150px; height: 160px;"'}
# htmltools::img(src = knitr::image_uri("forecastML_logo.png"), 
#                alt = 'logo', 
#                style = 'position:absolute; top:0; right:0; padding:5px; width: 150px; height: 160px;')
#knitr::include_graphics("forecastML_logo.png")
```

# Purpose

The purpose of `forecastML` is to provide a series of functions and visualizations that simplify the process of 
**multi-step-ahead direct forecasting with standard machine learning algorithms**. It's a wrapper package aimed at providing 
**maximum flexibility** in model-building--choose any machine learning algorithm from any `R` package--while helping 
the user quickly assess the (a) accuracy, (b) stability, and (c) generalizability of grouped (i.e., 
multiple related time-series) and ungrouped single-outcome forecasts produced from potentially high-dimensional modeling datasets.

This package is inspired by Bergmeir, Hyndman, and Koo's 2018 paper 
[A note on the validity of cross-validation for evaluating autoregressive time series prediction](https://robjhyndman.com/papers/cv-wp.pdf). 
In particular, `forecastML` makes use of 

* **lagged predictors**,
* **nested cross-validation** with (a) user-specified standard cross-validation in the inner loop and (b) block-contiguous validation 
datasets in the outer loop, and
* **parallel processing** with the `future` package 

to build and evaluate high-dimensional forecast models **without having to use methods that are time-series specific**. 
The following quote from Bergmeir et al.'s article nicely sums up the aim of this package:

> "When purely (non-linear, nonparametric) autoregressive methods are applied to forecasting problems, as is often the case
> (e.g., when using Machine Learning methods), the aforementioned problems of CV are largely
> irrelevant, and CV can and should be used without modification, as in the independent case."

## Direct forecasting

In contrast to the recursive or iterated method for producing multi-step-ahead forecasts used in traditional forecasting 
methods like ARIMA, direct forecasting involves creating a series of distinct horizon-specific models. Though 
several hybrid methods exist for producing multi-step forecasts, the simple direct forecasting method 
with lagged features used in `forecastML` let's us avoid the exponentially more difficult problem of having to 
"predict the predictors" for forecast horizons beyond 1-step-ahead.

Below are some resources for learning more about multi-step forecasting strategies:

* [A review and comparison of strategies for multi-step-ahead time series forecasting based on the NN5 forecasting competition](https://arxiv.org/pdf/1108.3259.pdf)
* [A comparison of direct and iterated multistep AR methods for forecasting macroeconomic time series](https://www.princeton.edu/~mwatson/papers/hstep_3.pdf)

The **animation below** shows how historical data is used to create a 1-to-12-step-ahead forecast for a 12-step-horizon 
forecast model using lagged features. Though predictor lags greater than 12 steps can be used to make use of 
additional historical predictive information, a 12-step-horizon direct forecast model requires predictor lags >= 12. 
This animation is roughly equivalent to how a 12-period seasonal ARIMA(0, 0, 0)(1, 0, 0) model uses historical data to produce forecasts.

```{r, eval = FALSE, echo = FALSE, out.width="600px", out.height="400px"}
knitr::include_graphics("direct_forecasting.gif")
```

![](direct_forecasting.gif){width=600px, height=400px}

<br>

# Package Features

1. **Transform datasets** for modeling by creating various patterns of lagged predictors for user-specified 
forecast horizons with `forecastML::create_lagged_df`

2. Create datasets for evaluating forecast models using **nested cross-validation** with `forecastML::create_windows`

3. Train and **evaluate machine learning models for forecasting** with `forecastML::train_model`.

4. Assess **forecast accuracy** at different forecast horizons with `forecastML::return_error`.

5. Assess **hyperparameter stability** with `forecastML::return_hyper`.

6. Create datasets of lagged predictors for **direct forecasting**.



```{r, include = FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.height = 4)
```

# Example

In this walkthrough of `forecastML` we'll compare the forecast performance of two machine learning 
methods, LASSO and Random Forest, across forecast horizons using the Seatbelts dataset from the `dataset` package.

Here's a summary of the problem at hand:

* **Outcome:**
    * `DriversKilled` - car drivers killed per month in the UK.
* **Predcitors:**
    * `DriversKilled` - car drivers killed per month in the UK.
    * `kms` - a measure of distance driven.
    * `PetrolPrice` - the price of gas.
    * `law` - A binary indicator of the presence of a seatbelt law.
* **Forecast:**
    * Model training - The first 15 years of the monthly dataset.
    * Model testing - The last year of the monthly dataset.

## Install forecastML

```{r, eval = FALSE}
install.packages("forecastML")
```


## Load packages and data

```{r, warning = FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(forecastML)
library(glmnet)
library(randomForest)
library(DT)

data("data_seatbelts", package = "forecastML")
data <- data_seatbelts

data <- data[, c("DriversKilled", "kms", "PetrolPrice", "law")]
DT::datatable(head(data, 5))
```


## Train-test split

```{r}
ts_frequency <- 12  # monthly time-series

data_train <- data[1:(nrow(data) - ts_frequency), ]
data_test <- data[(nrow(data) - ts_frequency + 1):nrow(data), ]

p <- ggplot(data, aes(x = 1:nrow(data), y = DriversKilled))
p <- p + geom_line()
p <- p + geom_vline(xintercept = nrow(data_train), color = "red", size = 1.1)
p <- p + theme_bw() + xlab("Index")
p
```



<br>

# Data Preparation

We'll create a list of datasets, one for each forecast horizon, with lagged values for each predictor. 
The `lookback` argument in `forecastML::create_lagged_df()` specifies the predictor lags in dataset rows.

```{r}
horizons <- 1:ts_frequency
lookback <- 1:15

data_list <- forecastML::create_lagged_df(data_train, type = "train",
                                          outcome_cols = 1, lookback = lookback,
                                          horizons = horizons)
```



<br>

Let's view the modeling dataset for a forecast horion of 6.

```{r}
DT::datatable(head(data_list[[6]], 10), options = list(scrollX = TRUE))
```



<br>

The plot below illustrates, for a given predictor, the number and position (in dataset rows) of lagged predictors 
created for each forecast horizon/model. The `lookback` argument to `forecastML::created_lagged_df()` was set to 
create lagged predictors from a minimum of 1 lag to a maximum of 15 lags; however, predictor lags that don't allow 
for direct forecasting at a given forecast horizon are removed from the modeling dataset.

```{r}
plot(data_list)
```



<br>

## Nested cross-validation

`forecastML::create_windows()` creates indices for partitioning the training dataset in the outer loop of a nested 
cross-validation setup. The validation datasets are created in contiguous blocks of `window_length`, as opposed to 
randomly seleted rows, to mimic forecasting over multi-step-ahead forecast horizons. The `skip`, 
`window_start`, and `window_stop` arguments take dataset indices that allow the user to adjust the number and 
placement of outer loop validation datasets.

```{r, warnings = FALSE, message = FALSE}
windows <- forecastML::create_windows(lagged_df = data_list, window_length = 12, skip = 0,
                                      window_start = NULL, window_stop = NULL,
                                      include_partial_window = TRUE)
windows
```



<br>

Below is a plot of the nested cross-validation outer loop datasets or windows. In our example, 
a `window_length` of 12 resulted in 14 validation windows.

In this nested cross-validation setup, a model is trained with data from 13 windows and 
forecast accuracy is assessed on the left out window. This means that we'll need to train 14 models, 
each selecting different optimal hyperparameters and model coefficients--if available--from 
the inner validation loop.

```{r}
plot(windows, data_list, show_labels = TRUE)
```



<br>

# Model Training

## User-defined forecast function

We'll compare the forecasting performance of two models: (a) a cross-validated LASSO and (b) a non-tuned Random Forest. 
The following user-defined functions are needed for each model:

* A wrapper function that takes the following positional **arguments**:
    * **1:** The input dataset with both target and model predictors. The predictor lags will be created according to the 
    `forecastML::create_lagged_df()` function.
    * **2:** The column index of the outcome to be forecasted. *Only 1 outcome can be modeled at present*.
* and **returns** a fitted model suitable for a `predict()`-type function.

Any inner loop cross-validation procedure should take place within this function, with the limitation that 
the inner cross-validation needs to ultimately `return()` one model.

```{r}
# Example 1 - LASSO
model_function <- function(data, outcome_cols = 1) {

  x <- data[, -(outcome_cols), drop = FALSE]
  y <- data[, outcome_cols, drop = FALSE]
  x <- as.matrix(x, ncol = ncol(x))
  y <- as.matrix(y, ncol = ncol(y))

  model <- glmnet::cv.glmnet(x, y)
  return(model)
}

# Example 2 - Random Forest
model_function_2 <- function(data, outcome_cols = 1) {

  outcome_names <- names(data)[outcome_cols]
  model_formula <- formula(paste0(outcome_names,  "~ ."))

  model <- randomForest::randomForest(formula = model_formula, data = data, ntree = 200)
  return(model)
}
```

<br>

## forecastML::train_model

For each modeling approach, LASSO and Random Forest, a total of `N forecast horizons` * `N validation windows` 
models are trained. In this example, that means training **`r length(data_list) * nrow(windows[[1]])` models** 
for each algorithm.

```{r}
model_results <- forecastML::train_model(lagged_df = data_list, windows, 
                                         model_function, model_name = "LASSO")
model_results_2 <- forecastML::train_model(lagged_df = data_list, windows, 
                                           model_function_2, model_name = "RF")
```

<br>

## User-defined prediction function

The following user-defined prediction function is needed for each model:

* A wrapper function that takes the following positional **arguments**:
    * **1:** The model returned from the user-defined modeling function.
    * **2:** A `data.frame()` of the model predictors. Do not manually create the lagged predictors.
* and **returns** a `data.frame()` of predictions with 1 column for each forecast target (limit 1 at present).

```{r}
# Example 1 - LASSO
prediction_function <- function(model, data_features) {

  x <- as.matrix(data_features, ncol = ncol(data_features))

  data_pred <- data.frame("y_pred" = predict(model, x, s = "lambda.min"))
  return(data_pred)
}

# Example 2 - Random Forest
prediction_function_2 <- function(model, data_features) {

  data_pred <- data.frame("y_pred" = predict(model, data_features))
  return(data_pred)
}
```

<br>

## forecastML::predict

The `predict.forecast_model()` method takes any number of trained models from `forecastML::train_model()` and a 
list of user-defined prediction functions. The list of prediction functions should appear in the same order 
as the models.

Outer loop nested cross-validation forecasts are returned for each user-defined model, forecast horizon, and 
validation window.

```{r}
data_results <- predict(model_results, model_results_2,
                        prediction_function = list(prediction_function, prediction_function_2))

```

<br>

Let's view the models' predictions. The data.frame with S3 class `training_results` contains the following columns:

* **model:** User-defined model name.
* **horizon:** Forecast horizon.
* **window_length:** Number of dataset rows in each validation window (partial windows have the user-specified window_length).
* **valid_indices:** The dataset row indices.
* **\<outcome>:** The name of the column being forecasted.
* **\<outcome_pred>:** The forecasts.

```{r}
DT::datatable(head(data_results, 10), options = list(scrollX = TRUE))
```



<br>

Below is a plot of the forecasts for each validation window at select forecast horizons.

```{r}
plot(data_results, type = "prediction", horizons = c(1, 6, 12))
```



<br>

Below is a plot of the forecast error for select validation windows at select forecast horizons.

```{r}
plot(data_results, type = "residual", horizons = c(1, 6, 12), windows = 10:14)
```



<br>

The plots below are diagnostic plots to check how forecasts for a target point in time have 
changed at different forecast horizons. In most cases it would be reasonable to expect 
shorter-horizon forecasts to be more accurate than longer-horizon forecasts.

* **Top plot:** Rolling origin forecasts for the last validation window in our training data.
* **Bottom plot:** Rolling origin forecasts for the first three points in our training data.

```{r}
plot(data_results, type = "forecast_stability", windows = max(data_results$window_number))
plot(data_results, type = "forecast_stability", valid_indices = attributes(data_list)$row_indices[1:3])
```



<br>

The `forecast_variability` plot below is a summary of the `forecast_stability` plot. It's a plot 
of the variability of forecasts for a target point in time collapsed across forecast horizons. A 
forecast model that produces greater variability across forecast horizons could be the better model 
provided the forecasts are increasingly accruate at shorter and shorter forecast horizons.

```{r}
plot(data_results, type = "forecast_variability", valid_indices = 30:80)
```



<br>

# Model Performance

## forecastML::return_error

Let's calcuate several common forecast error metrics.

* **mae:** Mean absolute error
* **mape:** Mean absolute percentage error
* **smape:** Symmetrical mean absolute percentage error from (Chen and Yang's 2004 formula with a 100% multiplier as 
discussed at [https://robjhyndman.com/hyndsight/smape/](https://robjhyndman.com/hyndsight/smape/))

The error for nested cross-validation are returned at 3 levels of granularity:

1. Error by validation window
2. Error by forecast horizon, collapsed across validation windows
3. Golbal error collapsed across validation windows and horizons

```{r}
data_error <- forecastML::return_error(data_results, metrics = c("mae", "mape", "smape"),
                                       models = NULL)

DT::datatable(data_error$error_global, options = list(scrollX = TRUE))
```
 
 
 
<br>
 
Below is a plot of error metrics across time for select validation windows and forecast horizons.

```{r}
plot(data_error, data_results, type = "time", horizons = c(1, 6, 12), windows = 10:14)
```



<br>

Below is a plot of forecast error metrics for each validation window (light) and the average across 
validation windows (dark).

```{r}
plot(data_error, data_results, type = "horizon", horizons = c(1, 6, 12))
```



<br>

Below is a plot of error metrics collapsed across validation windows and forecast horizons.

```{r}
plot(data_error, data_results, type = "global")
```



<br>

# Hyperparameters

While it may be reasonable to have distinct models for each forecast horizon or even forecasting model 
ensembles across horizons, at this point we still have slightly different LASSO and Random Forest models 
from the outer loop of the nested cross-validation *within* each horizon-specific model. Here, we'll 
take a look at the stability of the hyperparameters for the LASSO model to better understand if 
we can train one model across forecast horizons or if we need additional predictors or modeling 
strategies to forecast well under various conditions or time-series dynamics.

### User-defined hyperparameter function

The following user-defined hyperparameter function is needed for each model:

* A wrapper function that takes the following positional **arguments**
    * **1:** The model returned from the user-defined modeling function.
* and **returns** a `data.frame()` of predictions with 1 column for each forecast outcome.

```{r}
hyper_function <- function(model) {

  lambda_min <- model$lambda.min
  lambda_1se <- model$lambda.1se

  data_hyper <- data.frame("lambda_min" = lambda_min, "lambda_1se" = lambda_1se)
  return(data_hyper)
}
```

<br>

## forecastML::return_hyper

Below are two plots which show (a) univariate hyperparameter variability across the training data 
and (b) the relationship between each error metric and hyperparameter values.

```{r}
data_hyper <- forecastML::return_hyper(model_results, hyper_function)

plot(data_hyper, data_results, data_error, type = "stability", horizons = c(1, 6, 12))
plot(data_hyper, data_results, data_error, type = "error", c(1, 6, 12))
```



<br>

# Forecast

## forecastML::create_lagged_df

To forecast with the direct forecasting method, we need to create another dataset of lagged predictors. 
We can do this by running `create_lagged_df()` and setting `type = "forecast"`.

For non-grouped time-series, this function takes the last rows of data_train and creates lagged predictors that allow forecasting from 
1-step-ahead to *N* horizons for each horizon-specific model. Below is the forecast dataset for a 6-step-ahead forecast.

The `row_number` column gives the row index for the most recent period of actuals. In the example below, the `row_number` 180 
represents the last row in our model-building dataset `data_train`.

```{r}
data_forecast_list <- forecastML::create_lagged_df(data_train, type = "forecast", 
                                                  lookback = lookback,  horizon = horizons)

DT::datatable(head(data_forecast_list$horizon_6), options = list(scrollX = TRUE))
```



<br>

## Forecast results

Running the predict method, `predict.forecast_model()`, on the lagged predictor dataset created 
above--with `type = "forecast"`--and placing it in the `data_forecast` argument in `predict.forecast_model()` below, returns 
a data.frame of forecasts with the following columns:

* **model:** User-defined model name.
* **model_forecast_horizon:** The forecast horizon that the model was trained on.
* **horizon:** Forecast horizon, ranging from 1 to model_forecast_horizon.
* **window_length:** Number of dataset rows in each validation window (partial windows have the user-specified window_length).
* **window_number:** The validation window number.
* **forecast_period:** The dataset row/date for the forecast.
* **\<outcome_pred>:** The forecasts.

An S3 object of class, `forecast_results`, is returned. This object will have different plotting and error methods than 
the `training_results` class from earlier.

```{r}
data_forecast <- predict(model_results, model_results_2,
                         prediction_function = list(prediction_function, prediction_function_2), 
                         data_forecast = data_forecast_list)

DT::datatable(head(data_forecast, 10), options = list(scrollX = TRUE))
```



<br>

Below is a plot of the forecasts vs. the actuals from data_test for each model at select forecast horizons.

It's clear from the plots that our Random Forest model is producing less accurate forecasts and is more 
sensitive to the data on which it was trained--producing a handful of erratic forecasts.

```{r}
plot(data_forecast, data_actual = data_train[-(1:150), ],
     actual_indices = as.numeric(row.names(data_train[-(1:150), ])),
     horizons = c(1, 6, 12), facet_plot = c("model", "model_forecast_horizon"))

plot(data_forecast, data_actual = data_test, 
     actual_indices = as.numeric(row.names(data_test)),
     facet_plot = "model", horizons = c(1, 6, 12))
```



<br>

## Forecast error

Finally, we'll look at the forecast error by forecast horizon for our two models.

If the first argument of `forecastML::return_error()` is an object of class `forecast_results` and 
the `data_test` argument is a data.frame like data_test from our beginning train-test split, a data.frame 
of forecast error metrics with the following columns is returned:

* **model:** User-defined model name.
* **model_forecast_horizon:** The forecast horizon that the model was trained on.
* **horizon:** Forecast horizon, ranging from 1 to model_forecast_horizon.
* **\<error_metrics>:** Forecast error metrics.

```{r}
data_error <- forecastML::return_error(data_forecast, data_test = data_test,
                                       test_indices = as.numeric(row.names(data_test)),
                                       metrics = c("mae", "mape", "smape", "mdape"))

DT::datatable(head(data_error$error_by_horizon, 10), options = list(scrollX = TRUE))
```



<br>

# Model Selection and Re-training

Because our LASSO model is both stabler and more accurate, we'll re-train our model across 
the entire training dataset to get our final `r length(horizons)` models--1 for each forecast horizon. 
Note that for a real-world forecasting problem this is when we would do additional model tuning 
to imrpove forecast accuracy across validation windows as well as narrow the hyperparameter search 
in the user-specified modeling functions.

## forecastML::create_lagged_df

```{r}
data_list <- forecastML::create_lagged_df(data_train, type = "train", 
                                          lookback = lookback, 
                                          horizon = horizons)
```

<br>

## forecastML::create_windows

To create a dataset *without nested cross-validation*, set `window_length = 0` in `forecastML::create_windows()`.

```{r}
windows <- forecastML::create_windows(data_list, window_length = 0)

plot(windows, data_list, show_labels = TRUE)
```



<br>

## forecastML::train_model

Without nested cross-validation and holdout windows, the prediction plot is essnetially a plot of model fit.

```{r}
model_results <- forecastML::train_model(data_list, windows, model_function, model_name = "LASSO")

data_results <- predict(model_results, prediction_function = list(prediction_function))

DT::datatable(head(data_results, 10), options = list(scrollX = TRUE))
plot(data_results, type = "prediction", horizons = c(1, 6, 12))
plot(data_results, type = "residual", horizons = c(1, 6, 12))
plot(data_results, type = "forecast_stability", valid_indices = 109:120)
```



<br>

## forecastML::return_error

```{r}
data_error <- forecastML::return_error(data_results, metrics = c("mae", "mape", "mdape", "smape"),
                                       models = NULL)

DT::datatable(head(data_error$error_global), options = list(scrollX = TRUE))
plot(data_error, data_results, type = "horizon")
```



## forecastML::return_hyper

```{r}
data_hyper <- forecastML::return_hyper(model_results, hyper_function)

plot(data_hyper, data_results, data_error, type = "stability", horizons = c(1, 6, 12))
plot(data_hyper, data_results, data_error, type = "error", c(1, 6, 12))
```



<br>

## Forecast

```{r}
data_forecast_list <- forecastML::create_lagged_df(data_train, type = "forecast", 
                                                  lookback = lookback,  horizon = horizons)

data_forecast <- predict(model_results, prediction_function = list(prediction_function), 
                         data_forecast = data_forecast_list)

plot(data_forecast, data_actual = data[-(1:150), ],
     actual_indices = as.numeric(row.names(data[-(1:150), ])),
     horizons = c(1, 6, 12), 
     facet_plot = c("model", "model_forecast_horizon")) + ggplot2::theme(legend.position = "none")

plot(data_forecast, data_actual = data_test, actual_indices = as.numeric(row.names(data_test)),
     facet_plot = NULL, horizons = c(1, 6, 12))
```



<br>

## Forecast error

```{r}
data_error <- forecastML::return_error(data_forecast, data_test = data_test, 
                                       test_indices = as.numeric(row.names(data_test)),
                                       metrics = c("mae", "mape", "mdape", "smape"))

DT::datatable(data_error$error_by_horizon, options = list(scrollX = TRUE))
DT::datatable(data_error$error_global, options = list(scrollX = TRUE))
```

