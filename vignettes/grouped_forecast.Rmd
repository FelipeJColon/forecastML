---
title: "Forecasting with Multiple Time-Series"
author: "Nickalus Redell, nickalusredell@gmail.com"
date: "`r lubridate::today()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Forecasting with Multiple Time-Series}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.height = 4)
```

## Purpose

The purpose of this vignette is to provide a quick overview of forecasting with multiple time-series in 
`forecastML`. The benefits to modeling multiple time-series in one go with a single model or ensemble of 
models include (a) modeling simplicity, (b) potentially more robust results from pooling data across time-series, 
and (c) solving the cold-start problem when few data points are available for a given time-series.

We won't concern ourselves with error metrics or hyperparameter evaluation which can be assessed with the 
`forecastML::return_error()` and `forecastML::return_hyper()` functions. This functionality is covered 
in the package overview vignette.

***

## Setup

To forecast with multiple/grouped/hierarchical time-series in `forecastML`, your data needs the following 
characteristics:

* The **same outcome** is being forecasted across time-series.

* Data are in a **long format** with a single outcome column--i.e., time-series are stacked on top of each other 
in a data.frame.

* There are 1 or more **grouping columns**.

* There **may be 1 or more static features** that are constant through time but differ between time-series--e.g., 
a fixed location, store square footage, species of animal etc.

* The **time-series are regularly spaced** and have no missing rows or gaps in time. Irregular or sparse time 
series with many `NA`s *can* be modeled in this framework, but missing rows will result in incorrect feature 
lags when using `forecastML::create_lagged_df()` which is the first step in the `forecastML` workflow. 
At present, there is no helper function in `forecastML` to fill in missing temporal gaps, but this functionality 
will be added in a future release.

***

## Example

To illustrate forecasting with multiple time-series, we'll use the `data_buoy` dataset that comes 
with the `forecastML` package. This dataset consists of daily sensor measurements of several environmental 
conditions collected by 14 buoys in Lake Michigan from 2012 through 2018. The data were obtained 
from NOAA's National Buoy Data Center available at https://www.ndbc.noaa.gov/ using the `rnoaa` package.

* **Outcome:** Average daily wind speed in Lake Michigan.

* **Forecast horizon:** Daily, 1 to 30 days into the future which is essentially January 2019 for this dataset.

* **Time-series:** 14 outcome time-series collected from buoys throughout Lake Michigan.

* **Model:** A single gradient boosted tree model with `xgboost`.

### Load packages and data

`data_buoy` consists of:

* `date`: A date column which will be removed for modeling.

* `wind_spd`: The outcome.

* `lat` and `lon`: Latitude and longitude which are features that are static or unchanging through time.

* `day` and `year`: Dynamic features encoding time.

* `air_temperature` and `sea_surface_temperature`: Dynamic features collected from the buoys.


```{r, message = FALSE, warning = FALSE}
library(dplyr)
library(DT)
library(ggplot2)
library(forecastML)
library(xgboost)

data("data_buoy", package = "forecastML")

data <- data_buoy

DT::datatable(head(data), options = list(scrollX = TRUE))
```

***

### Plot wind speed outcome

* The wind speed data has some gaps in it: Some buoys collected data throughout the year, others only 
during the summer months.

* Gaps in data collection are fine provided your forecast model can handle `NA` values in the 
outcome and in the features (or, more precisely, your user-defined model wrapper function addresses any 
difficulties that an algorithm may have with `NA` values), but the input dataset needs to 
have contiguous dates for the feature lags from `forecastML::create_lagged_df()` to be correct. 
`data_buoy` comes ready for modeling with days that had missing sensor readings already 
filled in with data for columns `date`, `buoy_id`, `lat`, and `lon`.

* Notice that buoy 45186 has only recently come online and would be difficult to forecast on its own.

```{r, message = FALSE, warning = FALSE}
p <- ggplot(data, aes(x = date, y = wind_spd, color = ordered(buoy_id), group = year))
p <- p + geom_line()
p <- p + facet_wrap(~ ordered(buoy_id), scales = "fixed")
p <- p + theme_bw() + theme(
  legend.position = "none"
) + xlab(NULL)
p
```

***

### Modeling setup

* We'll simply and incorrectly set our grouping column, `buoy_id`, to numeric to work smoothly with `xgboost`. 
Better alternatives include [feature embedding](https://arxiv.org/abs/1604.06737), 
[target encoding](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/target-encoding.html) 
(available in the `R` package `catboost`), or [mixed effects Random Forests](https://arxiv.org/pdf/1901.11279.pdf).

* To be clear, `buoy_id` is both (a) used to identify a specific time-series for creating lagged features 
and (b) used as a feature in the model.

```{r}
data$buoy_id <- as.numeric(factor(data$buoy_id))
```

<br>

* Inputs for dataset creation with `forecastML::create_lagged_df()`.

```{r}
dates <- data$date  # Grouped time-series forecasting requires dates.
data$date <- NULL  # Dates, however, don't need to be in the input data.

frequency <- "1 day"  # A string that works in base::seq(..., by = "frequency").

outcome_cols <- 1  # The column position of our 'wind_spd' outcome.

groups <- "buoy_id"  # 1 forecast for each group or buoy.

horizons <- c(1, 7, 30)  # Forecast 1, 7, and 30 days into the future.

lookback <- c(1:30, 360:370)  # Features from 1 to 30 days in the past and annually.

static_features <- c("lat", "lon")  # Features that do not change through time.
```

<br>

### Model training with nested CV

#### Training dataset

* We have 3 datasets for training models that forecast 1, 1 to 7, and 1 to 30 days into 
the future. We'll view the 1-day-ahead training data below.

```{r}
type <- "train"  # Create a model-training dataset.

data_train <- forecastML::create_lagged_df(data, type = type, outcome_cols = outcome_cols,
                                           horizons = horizons, lookback = lookback,
                                           groups = groups, static_features = static_features, 
                                           dates = dates, frequency = frequency)

print(paste0("The class of `data_train` is ", class(data_train)))

DT::datatable(head(data_train$horizon_1), options = list(scrollX = TRUE))
```

***

* The plot below shows the feature map across forecast horiozons. Here, we set all 
non-static features to have the same lags (refer to the custom lags vignette to see how this could be modified). 
Notice that features that don't support direct forecasting to the given horizon are silently dropped.

```{r, message = FALSE, warning = FALSE}
p <- plot(data_train)  # plot.lagged_df() returns a ggplot object.
p <- p + geom_tile(NULL)  # Remove the gray border for a cleaner plot.
p
```

***

#### CV setup

* We'll model with **7 validation datasets**. Our earliest weather recordings start on 1-1-2012. 
We'll model with the first 350 days of the year and then skip the last 15 days with the 
`skip = 15` argument. The model will be trained on these last 15 days of each year, but 
we won't evaluate our performance here.

```{r, message = FALSE, warning = FALSE}
windows <- forecastML::create_windows(data_train, window_length = 350, skip = 15,
                                      include_partial_window = FALSE)

p <- plot(windows, data_train) + theme(legend.position = "none")
p
```

***

* Now we'll use the `group_filter = "buoy_id == 1"` argument to get a closer look at 
1 of our 14 time-series. The user-supplied filter is passed to `dplyr::filter()` internally.

```{r, message = FALSE, warning = FALSE}
p <- plot(windows, data_train, group_filter = "buoy_id == 1") + 
  theme(legend.position = "none")
p
```

***

#### User-defined model function

* This function takes the **2 positional arguments** `data` (a data.frame object) and `outcome_col`.

* Notice that the `xgboost`-specific input datasets are created within this wrapper function.

```{r}
model_function <- function(data, outcome_col) {
  
  # xgboost cannot handle missing outcomes data so we'll remove it.
  data <- data[!is.na(data[, outcome_col]), ]

  # 1 fixed validation dataset for early stopping. Ideally, we'd have many. 
  # We'll use an 80/20 split.
  indices <- 1:nrow(data)
  
  set.seed(224)
  train_indices <- sample(1:nrow(data), ceiling(nrow(data) * .8), replace = FALSE)
  test_indices <- indices[!(indices %in% train_indices)]

  data_train <- xgboost::xgb.DMatrix(data = as.matrix(data[train_indices, 
                                                           -(outcome_col), drop = FALSE]),
                                     label = as.matrix(data[train_indices, 
                                                            outcome_col, drop = FALSE]))

  data_test <- xgboost::xgb.DMatrix(data = as.matrix(data[test_indices, 
                                                          -(outcome_col), drop = FALSE]),
                                    label = as.matrix(data[test_indices, 
                                                           outcome_col, drop = FALSE]))

  params <- list("objective" = "reg:linear")

  watchlist <- list(train = data_train, test = data_test)
  set.seed(224)
  model <- xgboost::xgb.train(data = data_train, params = params, 
                              max.depth = 8, nthread = 2, nrounds = 100,
                              metrics = "rmse", verbose = 0, 
                              early_stopping_rounds = 5, 
                              watchlist = watchlist)

  return(model)
}
```

<br>

#### Model training

* This should take ~2 to 3 minutes to train our '3 forecast horizons' * 
'7 validation datasets' = *21 models*.

* The user-defined modeling wrapper function could be much more elaborate, in which case many more models 
could potentially be trained here.

* We'll train the models in parallel with the `use_future = TRUE` argument.

```{r, eval = FALSE}
future::plan(future::multiprocess)  # Multi-core or multi-session parallel training.

model_results_cv <- forecastML::train_model(lagged_df = data_train,
                                            windows = windows,
                                            model_function = model_function, 
                                            model_name = "xgboost",
                                            use_future = TRUE)
```

```{r, eval = FALSE, include = FALSE}
save(model_results_cv, file = "model_results_cv.rda")
```

```{r, include = FALSE}
load(file = "model_results_cv.rda")
```

```{r}
print(paste0("The class of `model_results_cv` is ", class(model_results_cv)))
```

<br>

* We can access the `xgboost` model for any horizon or validation window. Here, 
we show a `summary()` of the model for the first validation window which is 2012.

```{r}
summary(model_results_cv$horizon_1$window_1$model)
```

***

### Forecasting with nested models

* First, we'll visually evaluate our model's performance across our validation datasets.

* Then we'll forecast with each of our 21 models to get a sense of the stability of the forecasts 
produced from models trained on different subsets of our historical data.

#### User-defined predict function

* The function takes **2 positional arguments** and returns a 1-column data.frame. Future 
versions of `forecastML` will support returning lower and upper forecast bounds.

```{r}
prediction_function <- function(model, data_features) {
  x <- xgboost::xgb.DMatrix(data = as.matrix(data_features))
  data_pred <- data.frame("y_pred" = predict(model, x))
  return(data_pred)
}

prediction_function <- list(prediction_function)
```

<br>

#### Historical model fit.

* We're predicting on our validation datasets because we haven't specified the `data_forecast` argument 
in `predict()`

```{r}
data_pred_cv <- predict(model_results_cv, prediction_function = prediction_function)

print(paste0("The class of `data_pred_cv` is ", class(data_pred_cv)))
```

***

* The predictions are solid lines; the actuals are dashed lines.

* We'll filter this plot for closer inspection below.

```{r, message = FALSE, warning = FALSE}
plot(data_pred_cv) + theme(legend.position = "none")
```

***

* It's somewhat difficult to see how we've done here, so we'll use the `group_filter` argument again 
to focus on specific results. Note that the `plot()` function returns a ``ggplot` object so that we 
can easily modify our plot.

```{r, message = FALSE, warning = FALSE}
p <- plot(data_pred_cv, group_filter = "buoy_id %in% c(1, 2, 3)", horizons = 7) 
p <- p + theme(legend.position = "none")
p <- p + scale_x_date(limits = c(as.Date("2015-01-01"), as.Date("2015-12-31")))
p <- p + facet_grid(horizon ~ buoy_id) + ggtitle(paste0(p$labels$title, " and buoy ID"))
p
```

***

#### Forecasting

* We have 3 datasets that support forecasting 1, 1 to 7, and 1 to 30 days into 
the future. We'll view the 1-day-ahead forecasting data below.

```{r}
type <- "forecast"  # Create a forecasting dataset for our predict() function.

data_forecast <- forecastML::create_lagged_df(data, type = type, outcome_cols = outcome_cols,
                                           horizons = horizons, lookback = lookback,
                                           groups = groups, static_features = static_features, 
                                           dates = dates, frequency = frequency)

print(paste0("The class of `data_forecast` is ", class(data_forecast)))

DT::datatable(head(data_forecast$horizon_1), options = list(scrollX = TRUE))
```

***

* Now we'll forecast 1, 7, and 30 days into the future with `predict(..., data_forecast = data_forecast)`.

* The first time step into the future is `max(dates) + 1 * frequency`. Here, this is 
12-31-2018 + 1 * '1 day' or 1-1-2019.

```{r}
data_forecasts <- predict(model_results_cv, prediction_function = prediction_function, 
                          data_forecast = data_forecast)

print(paste0("The class of `data_forecasts` is ", class(data_forecasts)))
```

***

* Plots for each model--just `xgboost` here--and time horizon. The separate lines are for the 
combination of `buoy_id` * validation window.

```{r, message = FALSE, warning = FALSE}
plot(data_forecasts) + theme(legend.position = "none")
```

***

* We'll filter results to look at the forecasts across validation windows for `buoy_id == 1`. 
In this case, a validation window shows how the model would forecast if it were not trained 
on data in a given time frame.

```{r, message = FALSE, warning = FALSE}
plot(data_forecasts, group_filter = "buoy_id == 1")
```

***

### Model training all data

* The modeling steps are more or less the same as in the nested cross-validation modeling described 
above so we'll skip the explanations from here on out.

* Train across all data by setting `window_length = 0`.

```{r, message = FALSE, warning = FALSE}
windows <- forecastML::create_windows(data_train, window_length = 0)

p <- plot(windows, data_train) + theme(legend.position = "none")
p
```

***

```{r, eval = FALSE}
future::plan(future::multiprocess)

model_results_no_cv <- forecastML::train_model(lagged_df = data_train, 
                                               windows = windows,
                                               model_function = model_function, 
                                               model_name = "xgboost",
                                               use_future = TRUE)
```

```{r, eval = FALSE, include = FALSE}
save(model_results_cv, file = "model_results_no_cv.rda")
```

```{r, include = FALSE}
load(file = "model_results_no_cv.rda")
```

```{r}
print(paste0("The class of `model_results_no_cv` is ", class(model_results_no_cv)))
```

***

```{r}
data_forecasts <- predict(model_results_no_cv, prediction_function = prediction_function, 
                          data_forecast = data_forecast)

print(paste0("The class of `data_forecasts` is ", class(data_forecasts)))
```

***

```{r}
DT::datatable(head(data_forecasts), options = list(scrollX = TRUE))
```

***

```{r, message = FALSE, warning = FALSE}
plot(data_forecasts)
```

***

```{r, message = FALSE, warning = FALSE}
plot(data_forecasts, group_filter = "buoy_id == 1") + theme(legend.position = "none")
```

***
