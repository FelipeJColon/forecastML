% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/return_error.R
\name{return_error}
\alias{return_error}
\title{Compute forecast error}
\usage{
return_error(data_results, data_test = NULL, test_indices = NULL,
  metrics = c("mae", "mape", "mdape", "smape"), models = NULL,
  horizons = NULL, windows = NULL, group_filter = NULL)
}
\arguments{
\item{data_results}{An object of class 'training_results' or 'forecast_results'.}

\item{data_test}{If 'data_results' is an object of class 'forecast_results', a data.frame used to
assess the accuracy of a 'forecast_results' object. 'data_test' should have the outcome/target columns and any grouping columns.}

\item{test_indices}{Required if 'data_test' is given. A vector or 1-column data.frame of numeric row indices or dates (class'Date') with length nrow(data_test).}

\item{metrics}{Common forecast error metrics. See the Error Metrics section below for details.}

\item{models}{Filter results by user-defined model name from train_model() (optional).}

\item{horizons}{Filter results by horizon (optional).}

\item{windows}{Filter results by validation window number (optional).}

\item{group_filter}{A string for filtering plot results for grouped time-series (e.g., "group_col_1 == 'A'").}
}
\value{
A 'validation_error' or 'forecast_error' object: A list of data.frames
of error metrics for the validation datasets or forecast dataset depending
on the data_test argument. \cr
A list containing: \cr
Error metrics by horizon + validation window \cr
Error metrics by horizon, collapsed across validation windows \cr
Global error metrics collapsed across horizons and validation windows
}
\description{
Compute forecast error metrics on the validation datasets or a test dataset
}
\section{Error Metrics}{

mae = Means absolute error \cr
mape = Mean absolute percentage error \cr
mdape = Median absolute percentage error \cr
smape = Symmetrical mean absolute percentage error
}

\examples{
\donttest{
# Sampled Seatbelts data from the R package datasets.
data("data_seatbelts", package = "forecastML")

# Example - Training data for 2 horizon-specific models w/ common lags per predictor.
horizons <- c(1, 12)
lookback <- 1:15

data_train <- create_lagged_df(data_seatbelts, type = "train", outcome_cols = 1,
                               lookback = lookback, horizon = horizons)

windows <- create_windows(data_train, window_length = 12)

# User-define model - LASSO
# The model takes in a data.frame with a target and predictors with exactly the same format as
# in create_lagged_df(). 'outcome_cols' is the column index of the target. The
# model returns a model object suitable for a predict-type function.
library(glmnet)
model_function <- function(data, outcome_cols = 1) {

  x <- data[, -(outcome_cols), drop = FALSE]
  y <- data[, outcome_cols, drop = FALSE]
  x <- as.matrix(x, ncol = ncol(x))
  y <- as.matrix(y, ncol = ncol(y))

  model <- glmnet::cv.glmnet(x, y)
  return(model)
}

model_results <- train_model(data_train, windows,
                             model_function, model_name = "LASSO")

# User-defined prediction function - LASSO
# The predict() wrapper takes two positional arguments. First,
# the returned model from the user-defined modeling function (model_function() above).
# Second, a data.frame of predictors--lagged predictors will be created automatically
# using create_lagged_df().
prediction_function <- function(model, data_features) {

  x <- as.matrix(data_features, ncol = ncol(data_features))

  data_pred <- data.frame("y_pred" = predict(model, x, s = "lambda.min"))
  return(data_pred)
}

# Predict on the validation datasets.
data_valid <- predict(model_results, prediction_function = list(prediction_function))

# Forecast error metrics for validation datasets.
data_error <- return_error(data_valid)
}
}
